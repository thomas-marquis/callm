{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81e5264d-039c-402e-a50d-56b40d306dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "62ff47d5-ca4f-47cb-9cf0-ba6cfaae9677",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[30854,\n",
       " 841,\n",
       " 73953,\n",
       " 6502,\n",
       " 934,\n",
       " 35329,\n",
       " 379,\n",
       " 107902,\n",
       " 409,\n",
       " 7970,\n",
       " 4978,\n",
       " 6033,\n",
       " 409,\n",
       " 60014,\n",
       " 85,\n",
       " 22626,\n",
       " 15082,\n",
       " 1131]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a75a9c1-cb44-4896-929f-500d63a1d5cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ects/ai/callm/src/tokenizer/tokenizer.c:625 in byte_pair_encode(): Part found: v with id=85\n",
      "[DEBUG] /home/thomas/Documents/projects/ai/callm/src/tokenizer/tokenizer.c:620 in byte_pair_encode(): Part : aises (len=5)\n",
      "[DEBUG] /home/thomas/Documents/projects/ai/callm/src/tokenizer/tokenizer.c:625 in byte_pair_encode(): Part found: aises with id=22626\n",
      "[DEBUG] /home/thomas/Documents/projects/ai/callm/src/tokenizer/tokenizer.c:666 in Tokenizer_encode(): ========== ' situations' ===========\n",
      "[DEBUG] /home/thomas/Documents/projects/ai/callm/src/tokenizer/tokenizer.c:670 in Tokenizer_encode(): Token found:  situations with id=15082\n",
      "[DEBUG] /home/thomas/Documents/projects/ai/callm/src/tokenizer/tokenizer.c:666 in Tokenizer_encode(): ========== '...' ===========\n",
      "[DEBUG] /home/thomas/Documents/projects/ai/callm/src/tokenizer/tokenizer.c:670 in Tokenizer_encode(): Token found: ... with id=1131\n",
      "[INFO] /home/thomas/Documents/projects/ai/callm/src/tokenizer/tokenizer.c:233 in Tokenizer_new(): Creating encoder from file\n",
      "[INFO] /home/thomas/Documents/projects/ai/callm/src/tokenizer/tokenizer.c:241 in Tokenizer_new(): Creating encoder...\n",
      "[INFO] /home/thomas/Documents/projects/ai/callm/src/tokenizer/tokenizer.c:248 in Tokenizer_new(): Encoder created. Creating decoder\n",
      "[INFO] /home/thomas/Documents/projects/ai/callm/src/tokenizer/tokenizer.c:255 in Tokenizer_new(): Decoder created. Compiling ordinary tokens regex\n",
      "[INFO] /home/thomas/Documents/projects/ai/callm/src/tokenizer/tokenizer.c:334 in split_text(): Start encoding...\n",
      "[DEBUG] /home/thomas/Documents/projects/ai/callm/src/tokenizer/tokenizer.c:655 in Tokenizer_encode(): Printing text parts\n",
      "[DEBUG] /home/thomas/Documents/projects/ai/callm/src/tokenizer/tokenizer.c:656 in Tokenizer_encode(): Size: 15\n",
      "[DEBUG] /home/thomas/Documents/projects/ai/callm/src/tokenizer/tokenizer.c:659 in Tokenizer_encode(): First part: Je\n",
      "[DEBUG] /home/thomas/Documents/projects/ai/callm/src/tokenizer/tokenizer.c:666 in Tokenizer_encode(): ========== 'Je' ===========\n",
      "[DEBUG] /home/thomas/Documents/projects/ai/callm/src/tokenizer/tokenizer.c:670 in Tokenizer_encode(): Token found: Je with id=30854\n",
      "[DEBUG] /home/thomas/Documents/projects/ai/callm/src/tokenizer/tokenizer.c:666 in Tokenizer_encode(): ========== ' ne' ===========\n",
      "[DEBUG] /home/thomas/Documents/projects/ai/callm/src/tokenizer/tokenizer.c:670 in Tokenizer_encode(): Token found:  ne with id=841\n",
      "[DEBUG] /home/thomas/Documents/projects/ai/callm/src/tokenizer/tokenizer.c:666 in Tokenizer_encode(): ========== ' pense' ===========\n",
      "[DEBUG] /home/thomas/Documents/projects/ai/callm/src/tokenizer/tokenizer.c:670 in Tokenizer_encode(): Token found:  pense with id=73953\n",
      "[DEBUG] /home/thomas/Documents/projects/ai/callm/src/tokenizer/tokenizer.c:666 in Tokenizer_encode(): ========== ' pas' ===========\n",
      "[DEBUG] /home/thomas/Documents/projects/ai/callm/src/tokenizer/tokenizer.c:670 in Tokenizer_encode(): Token found:  pas with id=6502\n",
      "okenizer.c:666 in Tokenizer_encode(): ========== ' qu' ===========\n",
      "[DEBUG] /home/thomas/Documents/projects/ai/callm/src/tokenizer/tokenizer.c:670 in Tokenizer_encode(): Token found:  qu with id=934\n",
      "[DEBUG] /home/thomas/Documents/projects/ai/callm/src/tokenizer/tokenizer.c:666 in Tokenizer_encode(): ========== ''il' ===========\n",
      "[DEBUG] /home/thomas/Documents/projects/ai/callm/src/tokenizer/tokenizer.c:670 in Tokenizer_encode(): Token found: 'il with id=35329\n",
      "[DEBUG] /home/thomas/Documents/projects/ai/callm/src/tokenizer/tokenizer.c:666 in Tokenizer_encode(): ========== ' y' ===========\n",
      "[DEBUG] /home/thomas/Documents/projects/ai/callm/src/tokenizer/tokenizer.c:670 in Tokenizer_encode(): Token found:  y with id=379\n",
      "[DEBUG] /home/thomas/Documents/projects/ai/callm/src/tokenizer/tokenizer.c:666 in Tokenizer_encode(): ========== ' ait' ===========\n",
      "[DEBUG] /home/thomas/Documents/projects/ai/callm/src/tokenizer/tokenizer.c:670 in Tokenizer_encode(): Token found:  ait with id=107902\n",
      "[DEBUG] /home/thomas/Documents/projects/ai/callm/src/tokenizer/tokenizer.c:666 in Tokenizer_encode(): ========== ' de' ===========\n",
      "[DEBUG] /home/thomas/Documents/projects/ai/callm/src/tokenizer/tokenizer.c:670 in Tokenizer_encode(): Token found:  de with id=409\n",
      "[DEBUG] /home/thomas/Documents/projects/ai/callm/src/tokenizer/tokenizer.c:666 in Tokenizer_encode(): ========== ' bonnes' ===========\n",
      "[DEBUG] /home/thomas/Documents/projects/ai/callm/src/tokenizer/tokenizer.c:675 in Tokenizer_encode(): Token not found:  bonnes\n",
      "[DEBUG] /home/thomas/Documents/projects/ai/callm/src/tokenizer/tokenizer.c:570 in byte_pair_encode(): Pair:  b - Found in rank 293\n",
      "[DEBUG] /home/thomas/Documents/projects/ai/callm/src/tokenizer/tokenizer.c:570 in byte_pair_encode(): Pair: bo - Found in rank 754\n",
      "[DEBUG] /home/thomas/Documents/projects/ai/callm/src/tokenizer/tokenizer.c:570 in byte_pair_encode(): Pair: on - Found in rank 263\n",
      "[DEBUG] /home/thomas/Documents/projects/ai/callm/src/tokenizer/tokenizer.c:570 in byte_pair_encode(): Pair: nn - Found in rank 7521\n",
      "[DEBUG] /home/thomas/Documents/projects/ai/callm/src/tokenizer/tokenizer.c:570 in byte_pair_encode(): Pair: ne - Found in rank 818\n",
      "[DEBUG] /home/thomas/Documents/projects/ai/callm/src/tokenizer/tokenizer.c:570 in byte_pair_encode(): Pair: es - Found in rank 288\n",
      "[DEBUG] /home/thomas/Documents/projects/ai/callm/src/tokenizer/tokenizer.c:590 in byte_pair_encode(): Min rank: 263; Min size: 2; Parts len: 8\n",
      "[DEBUG] /home/thomas/Documents/projects/ai/callm/src/tokenizer/tokenizer.c:590 in byte_pair_encode(): Min rank: 288; Min size: 4; Parts len: 7\n",
      "[DEBUG] /home/thomas/Documents/projects/ai/callm/src/tokenizer/tokenizer.c:590 in byte_pair_encode(): Min rank: 293; Min size: 0; Parts len: 6\n",
      "[DEBUG] /home/thomas/Documents/projects/ai/callm/src/tokenizer/tokenizer.c:590 in byte_pair_encode(): Min rank: 4978; Min size: 2; Parts len: 5\n",
      "[DEBUG] /home/thomas/Documents/projects/ai/callm/src/tokenizer/tokenizer.c:590 in byte_pair_encode(): Min rank: 7970; Min size: 0; Parts len: 4\n",
      "[DEBUG] /home/thomas/Documents/projects/ai/callm/src/tokenizer/tokenizer.c:620 in byte_pair_encode(): Part :  bon (len=4)\n",
      "[DEBUG] /home/thomas/Documents/projects/ai/callm/src/tokenizer/tokenizer.c:625 in byte_pair_encode(): Part found:  bon with id=7970\n",
      "[DEBUG] /home/thomas/Documents/projects/ai/callm/src/tokenizer/tokenizer.c:620 in byte_pair_encode(): Part : nes (len=3)\n",
      "[DEBUG] /home/thomas/Documents/projects/ai/callm/src/tokenizer/tokenizer.c:625 in byte_pair_encode(): Part found: nes with id=4978\n",
      "[DEBUG] /home/thomas/Documents/projects/ai/callm/src/tokenizer/tokenizer.c:666 in Tokenizer_encode(): ========== ' ou' ===========\n",
      "[DEBUG] /home/thomas/Documents/projects/ai/callm/src/tokenizer/tokenizer.c:670 in Tokenizer_encode(): Token found:  ou with id=6033\n",
      "[DEBUG] /home/thomas/Documents/projects/ai/callm/src/tokenizer/tokenizer.c:666 in Tokenizer_encode(): ========== ' de' ===========\n",
      "[DEBUG] /home/thomas/Documents/projects/ai/callm/src/tokenizer/tokenizer.c:670 in Tokenizer_encode(): Token found:  de with id=409\n",
      "[DEBUG] /home/thomas/Documents/projects/ai/callm/src/tokenizer/tokenizer.c:666 in Tokenizer_encode(): ========== ' mauvaises' ===========\n",
      "[DEBUG] /home/thomas/Documents/projects/ai/callm/src/tokenizer/tokenizer.c:675 in Tokenizer_encode(): Token not found:  mauvaises\n",
      "[DEBUG] /home/thomas/Documents/projects/ai/callm/src/tokenizer/tokenizer.c:570 in byte_pair_encode(): Pair:  m - Found in rank 296\n",
      "[DEBUG] /home/thomas/Documents/projects/ai/callm/src/tokenizer/tokenizer.c:570 in byte_pair_encode(): Pair: ma - Found in rank 1764\n",
      "[DEBUG] /home/thomas/Documents/projects/ai/callm/src/tokenizer/tokenizer.c:570 in byte_pair_encode(): Pair: au - Found in rank 2933\n",
      "[DEBUG] /home/thomas/Documents/projects/ai/callm/src/tokenizer/tokenizer.c:570 in byte_pair_encode(): Pair: uv - Found in rank 12328\n",
      "[DEBUG] /home/thomas/Documents/projects/ai/callm/src/tokenizer/tokenizer.c:570 in byte_pair_encode(): Pair: va - Found in rank 6723\n",
      "[DEBUG] /home/thomas/Documents/projects/ai/callm/src/tokenizer/tokenizer.c:570 in byte_pair_encode(): Pair: ai - Found in rank 2192\n",
      "[DEBUG] /home/thomas/Documents/projects/ai/callm/src/tokenizer/tokenizer.c:570 in by"
     ]
    }
   ],
   "source": [
    "from pycallm.lib import pycallm\n",
    "\n",
    "encoder_file = \"../resources/tokenizer.model\"\n",
    "\n",
    "res = pycallm.tokenize(encoder_file, \"Je ne pense pas qu'il y ait de bonnes ou de mauvaises situations...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81f5072b-4fe5-4332-aa40-18556d96c5ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' bonne'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pycallm.get_token_by_id(encoder_file, 51651)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
